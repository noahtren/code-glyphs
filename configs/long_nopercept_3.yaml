# LOGGING AND LOGISTICS
local_prefix: ./
run_name: long_nopercept_3
use_gcs: true
gcs_prefix: gs://code-glyphs/
s3_bucket: gestalt-graph
gcs_bucket: code-glyphs
img_summaries_per_epoch: 6
img_summaries_every_n_epochs: 5

# MODELS
full_model: gestalt
full_models: [language, vision, gestalt]
eager_mode: false

# -- LANGUAGE
lang_autoencoder_r: 4 # 48x48
language_model_size: 768

# -- VISION
generator_model: cnn
generator_models: [cppn, cnn]
vision_model_size: 2048
vision_backbone: BiT-M-R50x1
generator_downsample_ratio: 2

# -- -- Generator
generator_model_size: 2048
generator_input_size: 2048
img_dim: 96
cppn_conv: true
cppn_blocks: 5
generator_levels: 5
res_blocks_per_level: 3
c_out: 3
composite_colors:
  - "ff0000" # primary
  - "00ff00"
  - "0000ff"
  - "ffff00" # secondary
  - "00ffff"
  - "ff00ff"
  - "ffffff" # bw
  - "000000"

# DATA
max_len: 128
num_symbols: 64

# alternate loss scores
use_perceptual_loss: false

# TRAINING
epochs: 500
batch_size: 80
lang_encoder_lr: 1.e-6 # BERT: peak value of 1-e4 with b=256
lang_decoder_lr: 1.e-6
generator_lr: 1.e-4
decoder_lr: 1.e-4
default_lr: 1.e-4
funnel_lr: 1.e-4
label_smoothing: 0.0001
use_aug: true
ckpt_every_n_epochs: 10

# learning rate scheduling
use_warmup: true
warmup_steps: 1000
use_decay: true
decay_rate: 0.975
decay_steps: 5000
